[GLOBALS]
# loss can be either cross_entropy or MSE
loss: cross_entropy
wreg: 0.001
wrt: L2


[INPUT LAYER]
size: 20


# hidden layer activation function available options: sigmoid | tanh | relu | linear
[HIDDEN LAYERS]
# format: size - activation - learning rate
1: 30 - relu - 0.1
2: 22 - relu - 0.1


[OUTPUT LAYER]
# output activation available options: softmax | none
activation: softmax


[DATA]
data: blabla
