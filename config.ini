[GLOBALS]
# loss can be either cross_entropy or MSE
loss: cross_entropy
wreg: 0.001
wrt: L2



# layer activation function available options: sigmoid | tanh | relu | linear
[LAYERS]
# format: size - activation - learning rate
i: 225 - none - none
1: 100 - sigmoid - 0.1
o: 4 - softmax - 0.1


[DATA]
data: blabla
