[GLOBALS]
# loss can be either cross_entropy or MSE
loss: cross_entropy
wreg: 0.001
# wrt: L1 | L2 | none
wrt: none



# layer activation function available options: sigmoid | tanh | relu | linear
[LAYERS]
# format: size - activation - learning rate
i: 225 - none - none
1: 100 - relu - 0.001
o: 4 - softmax - 0.001


[DATA]
data: blabla
