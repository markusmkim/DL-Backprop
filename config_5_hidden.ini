[GLOBALS]
# loss can be either cross_entropy or MSE
loss: cross_entropy
batch_size: 8
epochs: 10
verbose: false

# weight regularization
wreg: 0.05
# wrt: L1 | L2 | none
wrt: L2


# layer activation function available options: sigmoid | tanh | relu | linear
[LAYERS]
# format: size - activation - learning rate
i: 400 - none - none
1: 100 - sigmoid - 0.1
2: 50 - linear - 0.1
3: 50 - tanh - 0.1
4: 20 - sigmoid - 0.1
5: 20 - sigmoid - 0.1
o: 4 - softmax - 0.001


[DATA]
image_size: 20
centered: false
noise_rate: 0.05
number_of_images: 400
flatten: true
# share test = 1 - train - validate
share_train: 0.8
share_validate: 0.05